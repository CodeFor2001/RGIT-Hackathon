{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart attack possibility prediction \n",
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing\n",
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb;\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('heart.csv')\n",
    "s_dataset = dataset.sample(frac =1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#X = s_dataset.iloc[:, [0,1,2,3,4,5,6,7,8,9,10,11,12]].values\n",
    "#X = s_dataset.iloc[:, [0,1,2,7,8,9,10,11,12]].values\n",
    "X = s_dataset.drop('target',axis=1)\n",
    "y = s_dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dataset[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram with respect to age\n",
    "\n",
    "sb.FacetGrid(s_dataset, hue=\"target\", size = 6)\\\n",
    "  .map(sb.distplot, \"age\")\\\n",
    "  .add_legend();  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.FacetGrid(dataset, hue=\"target\", size = 6)\\\n",
    "  .map(sb.distplot, \"trestbps\")\\\n",
    "  .add_legend(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.FacetGrid(dataset, hue=\"target\", size = 6)\\\n",
    "  .map(sb.distplot, \"restecg\")\\\n",
    "  .add_legend(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.FacetGrid(dataset, hue=\"target\", size = 6)\\\n",
    "  .map(sb.distplot, \"thalach\")\\\n",
    "  .add_legend(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.FacetGrid(dataset, hue=\"target\", size = 6)\\\n",
    "  .map(sb.distplot, \"oldpeak\")\\\n",
    "  .add_legend(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.FacetGrid(dataset, hue=\"target\", size = 6)\\\n",
    "  .map(sb.distplot, \"slope\")\\\n",
    "  .add_legend(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x=\"target\", y=\"age\", data= dataset)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(x=\"sex\", y=\"target\", data= dataset)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def bar_plot(variable):\n",
    "    \"\"\"\n",
    "    input: variable ex: sex\n",
    "    output: barplot & value count\n",
    "    \"\"\"\n",
    "    # get features\n",
    "    var = s_dataset[variable]\n",
    "    # count number of categorical variable (value/sample)\n",
    "    varValue = var.value_counts()\n",
    "    # visualize\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plt.bar(varValue.index, varValue)\n",
    "    plt.xticks(varValue.index, varValue.index.values)\n",
    "    plt.ylabel(\"Frequanc\")\n",
    "    plt.title(variable)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"{}\\n{}\".format(variable, varValue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variable bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "for c in categorical:\n",
    "    bar_plot(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical variable bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(variable):\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plt.hist(s_dataset[variable], bins=40)\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel(\"Frequancy\")\n",
    "    plt.title(\"{} distribution with histogram\".format(variable))\n",
    "    plt.show()\n",
    "numericVar = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "for n in numericVar:\n",
    "    plot_hist(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By observing the kaggle dataset there are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot = sb.heatmap(dataset.corr(), linewidths=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML ALGORITHMS\n",
    "## Cross Validation Algorithms\n",
    "#### Decision tree classifier\n",
    "#### Support vector classifier\n",
    "#### Random Forest Classifier\n",
    "#### Logistic Regression\n",
    "#### KNeighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 , random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "classifier = [DecisionTreeClassifier(random_state=random_state),\n",
    "             SVC(random_state=random_state),\n",
    "             RandomForestClassifier(random_state=random_state),\n",
    "             LogisticRegression(random_state=random_state),\n",
    "             KNeighborsClassifier()]\n",
    "\n",
    "dt_param_grid = {\"min_samples_split\":range(10,500,20),\n",
    "                \"max_depth\":range(1,20,2)}\n",
    "\n",
    "svc_param_grid = {\"kernel\":[\"rbf\"],\n",
    "                 \"gamma\":[0.001,0.01,0.1,1],\n",
    "                 \"C\":[1,10,50,100,200,300,1000]}\n",
    "\n",
    "rf_param_grid = {\"max_features\":[1.3,10],\n",
    "                \"min_samples_split\":[2,3,10],\n",
    "                \"min_samples_leaf\":[1,3,10],\n",
    "                \"bootstrap\":[False],\n",
    "                \"n_estimators\":[100,300],\n",
    "                \"criterion\":[\"gini\"]}\n",
    "\n",
    "lr_param_grid = {\"C\":np.logspace(-3,3,7),\n",
    "                \"penalty\":[\"l1\",\"l2\"]}\n",
    "\n",
    "knn_param_grid = {\"n_neighbors\":np.linspace(1,19,10, dtype=int).tolist(),\n",
    "                 \"weights\":[\"uniform\",\"distance\"],\n",
    "                 \"metric\":[\"euclidean\",\"manhattan\"]}\n",
    "\n",
    "classifier_param = [dt_param_grid,\n",
    "                   svc_param_grid,\n",
    "                   rf_param_grid,\n",
    "                   lr_param_grid,\n",
    "                   knn_param_grid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names=[\"DecisionTree :\", \"SVC : \", \"RandomForest : \", \"LogisticRegression : \", \"KNN : \"]\n",
    "cv_result = []\n",
    "best_estimators = []\n",
    "for i in range(len(classifier)):\n",
    "    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv=StratifiedKFold(n_splits=10), scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    cv_result.append(clf.best_score_)\n",
    "    best_estimators.append(clf.best_estimator_)\n",
    "    print(model_names[i], cv_result[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_result)\n",
    "cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVC\", \"RandomForestClassifier\", \"LogisticRegression\", \"KNeigborsClassifier\"]})\n",
    "\n",
    "g = sb.barplot(x=\"Cross Validation Means\", y = \"ML Models\", data=cv_results)\n",
    "g.set_xlabel(\"Means Accuracy\")\n",
    "g.set_title(\"Cross Validation Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network(ANN)\n",
    "##  Building the ANN\n",
    "### Initialising the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=9, activation ='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "ann.add(tf.keras.layers.Dropout(0.45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation ='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "ann.add(tf.keras.layers.Dropout(0.52))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Training the ANN\n",
    "### Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate= 0.01)\n",
    "ann.compile( optimizer=optimizer , loss = 'binary_crossentropy' , metrics = ['accuracy']  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ANN on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.fit(X_train,y_train , validation_split=0.2,batch_size= 16 , verbose=2,shuffle = True,epochs  = 100)\n",
    "test_loss, test_acc = ann.evaluate(X_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n",
    "                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_predicted = xgb.predict(X_test)\n",
    "xgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\n",
    "xgb_acc_score = accuracy_score(y_test, xgb_predicted)\n",
    "print(\"confussion matrix\")\n",
    "print(xgb_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ev = pd.DataFrame({'Model': ['Extreme Gradient Boost','ANN'], 'Accuracy': [xgb_acc_score*100,test_acc*100]})\n",
    "model_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421052631578947"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('svr', make_pipeline(StandardScaler(),\n",
    "                           LinearSVC(random_state=42)))\n",
    " ]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression()\n",
    " )\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, stratify=y, random_state=42\n",
    " )\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = int(input(\"Enter your age: \"))\n",
    "sex = int(input(\"Enter 0 if you are female, 1 if you are male: \"))\n",
    "cp = int(input(\"Enter your chest pain type 1/2/3/4: \"))\n",
    "trestbps = int(input(\"Enter your resting blood pressure: \"))\n",
    "chol = int(input(\"Enter your serum cholestrol in mg/dl: \"))\n",
    "fbs = int(input(\"Enter 1 if your fbs is greater than 120mg/dl: \"))\n",
    "restecg = int(input(\"Enter your resting ecg 0/1/2: \"))\n",
    "thalach = int(input(\"Enter maximum heart rate achieved : \"))\n",
    "exang = int(input(\"Do you have exercise induced agnia? 0/1:  \"))\n",
    "oldpeak = float(input(\"Enter ST depression induced by exercise relative to rest?:   \"))\n",
    "slope = int(input(\"Slope of the peak exercise ST segment: 0/1/2:  \"))\n",
    "ca = int(input(\"number of major vessels (0-3): \"))\n",
    "thal = int(input(\"0 = normal; 1 = fixed defect; 2 = reversable defect:  \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ann.predict(sc.transform([[age,sex,cp,trestbps,chol,fbs,restecg,thalach,exang,oldpeak,slope,ca,thal]]))>0.5):\n",
    "    print(\"You have a possibility of heart attack\")\n",
    "else:\n",
    "    print(\"No worries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
